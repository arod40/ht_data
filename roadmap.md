1-Curate the HT dataset
2-Pre-train language models on unsupervised/semi-supervised tasks on this dataset
3-Define and train on fine-tuning tasks with labels that can be automatically generated, like tagging phone numbers for instance
    3.1-Idenfity indicators of independent workers (can be done programmatically? yes)
4-Define and train on tasks where degree of relatedness (graph distance for instance) is predicted, but indicators (like common phone numbers) are hidden/masked/added-noise-to
5-We add this predicted relatedness to a graph topology for visualization
6-Find ways of extraction attention information from transformers and visualizing on each task what the models are paying attention to to solve each task, particularly on task 4
    6.1-Time sequence analysis(es)

ADD DATES TO EACH STEP

---RELATE POSTS--- (read to find others)
same text
same phone

---CLUSTERING INDICATORS FOR TRAFFICKING--- (read to find others)
geo distance large (consider at same time)
same criminal circuits/tracks (given?) (pattern of moving add in time)


---LEARN NL INDICATORS THAT WOULD SIMULATE THOSE CLUSTERS---

---REVERSE ENGINEER NEW CLUSTERS WITH THOSE INDICATORS---
pull out new heuristics for tracks based on predictions for NL


---NEGATIVE CASES---
absence of suspicion indicators
charge less
less vulgar language
willingness to do dangaerous things



Find text similarities

Evaluation FW:

1-Implement streamlit labeling app
2-Deploy app: Baylor?
3-Implement evaluation app


FIND TRANSFORMERS CHARBASED TRAINED ON LARGE VOCAB (EMOJIS, CHINESE, LATIN) ---> MULTILINGUAL??

UTC: https://arxiv.org/abs/2202.11176
Charformer: https://arxiv.org/abs/2106.12672
Canine: https://arxiv.org/abs/2103.06874
ByteT5: https://arxiv.org/abs/2105.13626
